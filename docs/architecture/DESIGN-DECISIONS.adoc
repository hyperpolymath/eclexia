// SPDX-License-Identifier: PMPL-1.0-or-later
// SPDX-FileCopyrightText: 2025 Jonathan D.A. Jewell

= Eclexia: Design Decisions & Rationale
Jonathan D.A. Jewell <jonathan.jewell@open.ac.uk>
:toc: macro
:toc-title: Contents
:sectnums:
:icons: font

[abstract]
This document records the design decisions made during the development of
the Eclexia programming language compiler, runtime, and toolchain. Each
decision is tied to the development stage where it arose, the alternatives
considered, and the rationale for the chosen approach. This is intended
as a teaching resource for programming language (PL) classes and as a
permanent record of the project's architectural evolution.

toc::[]

== 1. Language Architecture Decisions

=== 1.1. Hand-Written Recursive Descent Parser (vs. Parser Generators)

**Stage:** Early compiler bootstrap (lexer + parser)

**Decision:** Write the parser by hand using recursive descent with Pratt
parsing for operator precedence.

**Alternatives considered:**

- *Parser generators (LALR/PEG):* Tools like lalrpop, pest, or tree-sitter
  would generate the parser automatically from a grammar specification.
- *Parser combinator libraries:* nom, chumsky, or winnow would let us compose
  parsers from smaller building blocks.

**Rationale:**

1. **Error recovery control.** Eclexia needs excellent error messages because
   it targets an audience that includes economists and policy analysts, not
   just programmers. Hand-written parsers give complete control over error
   recovery strategy -- we can insert synchronisation points at items,
   statements, and expressions independently. With a parser generator, error
   recovery is typically limited to the generator's built-in strategy.

2. **Incremental parsing.** The long-term plan is to integrate with the Salsa
   incremental compilation framework (eclexia-db). Hand-written parsers are
   easier to make incremental -- you can re-parse individual functions
   rather than re-running the entire grammar.

3. **Pratt parsing for expressions.** Eclexia has 16 precedence levels
   including resource-specific operators. Pratt parsing handles this
   elegantly with a simple precedence table, avoiding the deep grammar
   nesting that makes recursive descent inefficient for expressions.

**Lesson for students:** Parser generators are excellent for prototyping
and for languages with simple grammars. But production compilers (GCC, Clang,
Rust, Go, Swift) overwhelmingly use hand-written parsers because the control
over error recovery, incremental parsing, and performance is worth the
additional implementation effort.

=== 1.2. Error Recovery Strategy (Three-Level)

**Stage:** Parser maturation (after initial MVP worked but error messages
were poor)

**Decision:** Implement error recovery at three levels:

1. **Item level** (`recover_to_item`): On failure parsing a top-level item,
   skip tokens until reaching a keyword that starts a new item (`fn`, `def`,
   `type`, `struct`, `trait`, `impl`, `mod`, `use`, `const`, `static`,
   `extern`). Emit an `Item::Error(span)` placeholder node.

2. **Statement level** (`recover_to_stmt`): Inside blocks, skip to the
   next statement-starting keyword (`let`, `return`, `while`, `for`, `if`,
   `match`, `loop`, `break`, `continue`, `fn`, `def`, `struct`) or to a
   semicolon or closing brace. Emit a `Stmt::Error` placeholder.

3. **Expression level** (delimiter-aware): Inside parenthesised,
   bracketed, or braced groups, skip to the matching closing delimiter
   while tracking nesting depth. Emit an `ExprKind::Error` placeholder.
   This prevents a single bad expression inside `f(a, BAD, c)` from
   losing the entire function call.

**Alternatives considered:**

- *Single-level recovery:* Simpler but causes cascading errors.
  A syntax error inside a function body would prevent parsing all
  subsequent functions.
- *Token insertion/deletion:* More sophisticated (as in Roslyn/C#) but
  much more complex to implement correctly.

**Rationale:** The three-level approach gives 80% of the benefit of
full error recovery with 20% of the complexity. The delimiter-aware
expression recovery is particularly important because Eclexia has
resource literals (`100J`, `5ms`) that can confuse the parser if
a unit suffix is misspelled.

**Challenge encountered:** Adding `recover_to_closing_delimiter` required
carefully tracking nesting depth. A naive implementation that just scanned
for `)` would break on `f((a, b), c)` by stopping at the inner `)`.

=== 1.3. Pratt Parsing with 16 Precedence Levels

**Stage:** Expression parser design

**Decision:** Use Pratt (top-down operator precedence) parsing with an
explicit precedence enum:

[source,rust]
----
#[repr(u8)]
pub enum Precedence {
    None = 0,       Assignment = 1,  Or = 2,
    And = 3,        Equality = 4,    Cast = 5,
    Comparison = 6, BitOr = 7,       BitXor = 8,
    BitAnd = 9,     Shift = 10,      Term = 11,
    Factor = 12,    Power = 13,      Unary = 14,
    Call = 15,       Primary = 16,
}
----

**Rationale:** Eclexia's expression language includes:

- Standard arithmetic (`+`, `-`, `*`, `/`, `%`, `**`)
- Bitwise operations (`&`, `|`, `^`, `~`, `<<`, `>>`)
- Comparison and equality
- Logical operators (`and`/`or`/`not` as keywords)
- Resource operators (future: shadow price arithmetic)
- Type casts (`as`)
- Range operators (`..`, `..=`)
- Try operator (`?`)
- Await (`.await` as postfix)

Pratt parsing handles all of these with a flat loop rather than
deeply nested grammar rules. The `parse_expr_prec(min_prec)` function
is the core -- it calls `parse_prefix`, then loops calling `parse_infix`
as long as the next operator's precedence exceeds `min_prec`.

**Lesson for students:** Pratt parsing was invented by Vaughan Pratt
in 1973. It's equivalent to operator-precedence parsing but more
intuitive. The key insight is that each token has a "binding power"
(precedence), and we only parse the right side of an operator if
the next token's binding power is higher than the current operator's.

== 2. Type System Decisions

=== 2.1. Robinson Unification (Hindley-Milner variant)

**Stage:** Type checker implementation

**Decision:** Use Robinson's unification algorithm for type inference,
extended with resource types and dimensional analysis.

**Key type forms:**

- Primitives (`Int`, `Float`, `Bool`, `Char`, `String`, `Unit`)
- Resource types (`Resource<Energy>`, `Resource<Carbon>`)
- Function types (`fn(T, U) -> R`)
- Named types with generic parameters
- Type variables (for inference)
- An `Error` type for error recovery

**Lesson for students:** Robinson unification works by maintaining a
substitution (mapping from type variables to concrete types). When we
need to check that two types are equal, we "unify" them -- either
they're already equal, one is a variable (which gets bound), or
they're structurally recursive (unify components). The "occurs check"
prevents infinite types like `T = List<T>`.

=== 2.2. Resource Types with Dimensional Analysis

**Stage:** After basic type checker worked, extending to Eclexia's
unique resource tracking

**Decision:** Resources carry dimensional information in their type:
`Ty::Resource { base: PrimitiveTy, dimension: Dimension }`. Dimensions
are represented as vectors of exponents over base quantities:

- Energy: `kg * m^2 * s^-2` (Joules)
- Time: `s`
- Memory: `bytes`
- Carbon: `gCO2e`
- Power: `kg * m^2 * s^-3` (Watts)

**Rationale:** This lets the type checker catch dimensional errors
at compile time. Adding energy to time is a type error, not a runtime
surprise. The `as` cast operator allows explicit dimension conversion
(e.g., `energy_joules as Resource<Power>` with appropriate division
by time).

**Challenge:** Resource literals like `100J` need the lexer to recognise
unit suffixes and the type checker to map them to dimensions. We added
`parse_unit` in the AST crate that maps unit strings to `Dimension`
values.

== 3. Compiler Pipeline Decisions

=== 3.1. Five-Stage Pipeline (AST -> HIR -> MIR -> Bytecode -> VM)

**Stage:** Post-parser, designing the compilation pipeline

**Decision:** Use a multi-stage lowering pipeline:

1. **AST** (Abstract Syntax Tree): Faithful representation of source
   syntax. Arena-allocated with `ExprId`/`TypeId`/`StmtId` indices.
2. **HIR** (High-Level IR): Desugared, names resolved, types attached.
   `for` loops lowered to `loop`+`match`, method calls lowered to
   function calls with explicit receiver.
3. **MIR** (Mid-Level IR): Control flow graph with basic blocks.
   Explicit drops, borrows, and moves.
4. **Bytecode**: Stack-based instruction set with ~50 opcodes.
   Constant pool, function table, register-like locals.
5. **VM**: Stack-based virtual machine executing bytecode.
   934 lines, handles all core instructions.

**Alternative considered: Multi-language pipeline**

An alternative would be to use different languages at different
compilation stages, leveraging each language's strengths:

- *Idris2* for the front-end (dependent types for proving type system
  properties, formal verification of resource invariants)
- *Rust* for the middle-end and code generation (zero-cost abstractions,
  memory safety, LLVM bindings)
- *Elixir/BEAM* for the runtime (battle-tested actor scheduler, fault
  tolerance, hot code reload, distributed computing)

**Why we didn't do this:** The single-language approach (all Rust) was
chosen for pragmatic reasons:

1. **Toolchain complexity.** Each language boundary is a serialisation
   boundary. AST -> JSON -> Idris2 parse -> process -> JSON -> Rust
   parse adds latency and debugging difficulty.
2. **Build system.** A Rust-only workspace uses `cargo` natively.
   A multi-language pipeline needs custom orchestration.
3. **Developer onboarding.** Contributors need to know one language,
   not three.

**However**, the multi-language approach has real advantages:

- BEAM's scheduler is far more mature than anything we could build
- Idris2's dependent types could formally verify resource invariants
  that our Robinson unifier can only approximate
- Hot code reload on BEAM would be natural; in Rust it requires unsafe

**Lesson for students:** This is a classic "build vs. buy" decision.
The answer depends on your goals: if the language is a teaching tool
or research vehicle, single-language simplicity wins. If it's targeting
production distributed systems, leveraging BEAM's runtime might be worth
the toolchain complexity.

=== 3.2. Arena Allocation for AST Nodes

**Stage:** AST design

**Decision:** Use arena allocation (`la_arena::Arena`) for all AST
nodes. Expressions, types, and statements are allocated in arenas
and referenced by typed indices (`ExprId`, `TypeId`, `StmtId`).

**Rationale:**

1. **Cache locality.** All expressions are contiguous in memory.
   Tree walks are fast because the CPU cache is warm.
2. **Small references.** `ExprId` is a 32-bit index, not a 64-bit
   pointer. This halves the size of every AST node that references
   another node.
3. **No lifetime complexity.** Arena-allocated data lives as long as
   the arena. No need for `Box<Expr>` with recursive types or
   `Rc<RefCell<...>>` for shared ownership.

**Trade-off:** Arenas make it harder to modify the tree in place
(you can't easily "replace" a node). This is fine for a compiler
where each pass produces a new IR rather than mutating the old one.

=== 3.3. Separate Tree-Walking Interpreter Path

**Stage:** Early development, before bytecode compiler was ready

**Decision:** Maintain a tree-walking interpreter (`eclexia-interp`)
alongside the full compilation pipeline.

**Rationale:**

1. **Fast development cycle.** The interpreter was the first execution
   engine, built in parallel with the parser. It enabled testing
   language features before the bytecode compiler existed.
2. **REPL.** The REPL uses the interpreter for immediate evaluation.
   Compiling to bytecode for a single REPL expression is overkill.
3. **Debugging reference.** When the bytecode compiler produces wrong
   results, the interpreter serves as the "ground truth" implementation.

**Challenge:** Every new AST node must be handled in both the
interpreter and the bytecode compiler. The concurrency expressions
(`spawn`, `channel`, `send`, `recv`, `select`, `yield`) currently
return "not yet supported" errors in the interpreter but have proper
stub handling in the type checker and codegen.

== 4. Concurrency & Parallelism Decisions

=== 4.1. Tokio-Based Async Runtime with Resource Tracking

**Stage:** Advanced toolchain extensions (Section 17)

**Decision:** Build `eclexia-async` on top of tokio rather than
writing a custom async runtime. Wrap tokio primitives with resource
tracking (energy budget, carbon budget, shadow prices).

**Alternatives considered:**

- *Custom runtime from scratch:* Maximum control but massive effort.
  async/await polling, I/O reactor, timer wheel, work-stealing
  scheduler -- each is a multi-month project.
- *smol/async-std:* Lighter than tokio but less ecosystem support.
- *Structured concurrency (Trio-style):* No mature Rust implementation.

**Rationale:** Tokio is the de facto Rust async runtime with excellent
performance and ecosystem support. By wrapping it, we get:

- Multi-threaded work-stealing scheduler (free)
- Efficient channel implementations (MPSC, broadcast, oneshot)
- spawn_blocking for CPU-bound work
- Timer and I/O integration

We add Eclexia's unique features on top:

- `ResourceBudget` per task (energy, carbon, memory limits)
- `ShadowPrices` that influence scheduling decisions
- `RuntimeStats` for aggregate resource monitoring
- Carbon-aware scheduling (defer tasks during high-carbon periods)

**Lesson for students:** "Don't rewrite what you can wrap." The
most expensive code to maintain is code that reimplements well-tested
infrastructure. Focus your innovation budget on what's genuinely new
(resource-aware scheduling) rather than what's commodity (TCP polling).

=== 4.2. Fork-Join Parallel Iterators (vs. Rayon)

**Stage:** Implementing `par_map`/`par_filter`/`par_reduce`

**Decision:** Implement simple fork-join parallel iterators using
`std::thread::spawn` and chunking, rather than using rayon's
work-stealing parallel iterators.

**Rationale:** Eclexia's parallel iterators need to integrate with
the resource tracking system -- each chunk should track its own
energy/carbon usage. Rayon's iterators are opaque to this kind of
instrumentation. The simple fork-join model (split data into
N chunks, spawn N threads, join results) is:

1. **Transparent.** Each chunk is a known-size unit of work.
2. **Instrumentable.** We can wrap each chunk in resource tracking.
3. **Simple to understand.** Students can read the implementation.

**Trade-off:** Rayon's work-stealing is more efficient when tasks
have uneven sizes. Our simple chunking can leave threads idle if
one chunk takes much longer than others. This is acceptable for
an initial implementation -- we can add work-stealing later.

== 5. Incremental Compilation Decisions

=== 5.1. Salsa Framework for Demand-Driven Compilation

**Stage:** Building reactive compiler infrastructure

**Decision:** Use the Salsa framework (developed for rust-analyzer)
for incremental compilation. Each compilation step is a "tracked
query" that Salsa memoises and invalidates automatically when inputs
change.

**Queries:**

- `parse(db, file) -> AstWrapper` (memoised)
- `type_check(db, file) -> DiagnosticsWrapper` (depends on parse)
- `lower_hir(db, file) -> HirWrapper` (depends on parse)
- `lower_mir(db, file) -> MirWrapper` (depends on lower_hir)
- `generate_bytecode(db, file) -> BytecodeWrapper` (depends on lower_mir)

**Rationale:** Salsa gives us:

1. **Automatic dependency tracking.** If we change file A, Salsa knows
   which queries depend on A and which can be reused from cache.
2. **Demand-driven evaluation.** Queries are only computed when needed.
   If we only need diagnostics (for the LSP), we don't generate bytecode.
3. **Generation tracking.** Each result carries a generation number,
   allowing downstream consumers to check if their inputs changed.

**Challenge:** Wiring Salsa into the existing CLI pipeline required
adding the eclexia-db crate as a dependency and routing compilation
through `CompilerDatabase` instead of calling the pipeline directly.
This is a non-trivial refactor because the existing pipeline passes
owned data between stages, while Salsa works with shared references.

=== 5.2. File Watching with Debounced Batching

**Stage:** Watch mode for incremental compilation

**Decision:** Use the `notify` crate for file system watching with
custom debounced change batching. Changes are collected over a short
window (default 100ms) then processed as a batch.

**Rationale:** File system events are noisy -- saving a file in an
editor often generates multiple events (write, close, chmod). Without
debouncing, each event triggers a full recompilation. With batching,
we collect all events in the window and deduplicate them.

== 6. Toolchain Decisions

=== 6.1. Bytecode Disassembler

**Stage:** Debugging and transparency tooling

**Decision:** Add an `eclexia disasm` command that displays human-readable
bytecode for a compiled Eclexia program. Shows instruction mnemonics,
operand values, constant pool entries, and function boundaries.

**Rationale:** A disassembler is essential for:

1. **Compiler debugging.** Verifying that the code generator produces
   correct bytecode for a given source program.
2. **Performance investigation.** Understanding why a program is slow
   by examining the generated instruction sequence.
3. **Teaching.** Showing students what a high-level program looks like
   after compilation to a stack-based instruction set.

=== 6.2. REPL Design (rustyline + Interpreter)

**Stage:** Developer experience tooling

**Decision:** Build the REPL on rustyline with the tree-walking
interpreter (not the bytecode compiler).

**Rationale:** REPLs need sub-second response times. The interpreter
evaluates expressions immediately without the overhead of HIR lowering,
MIR lowering, bytecode generation, and VM execution. The REPL also
supports special commands (`:type`, `:shadow`, `:resources`) that
query the type checker or runtime directly.

== 7. Metacompiler Considerations

=== 7.1. Why Not a Metacompiler?

**Stage:** Advanced architecture discussion

A *metacompiler* (or compiler-compiler) generates a compiler from a
specification of the language. This is distinct from a parser generator --
it generates the entire compilation pipeline from interpreter annotations
or grammar specifications.

**Approaches evaluated:**

- **Nanopass architecture** (Sarkar et al., 2004): Decompose the
  compiler into dozens of tiny passes, each doing one small transformation.
  Each pass is small enough to formally verify. Chez Scheme uses this
  internally. This is the most practical option for Eclexia -- the
  existing HIR/MIR lowering passes could be decomposed into smaller,
  provable steps.

- **Truffle/GraalVM metacompilation**: Write an annotated interpreter
  and let the framework automatically generate a JIT compiler from it.
  This is how TruffleRuby and GraalPython achieve near-C performance.
  For Eclexia, this could replace the stub backends (Cranelift, LLVM,
  WASM) with a single metacompiled approach.

- **Futamura projections**: The theoretical foundation. The first
  Futamura projection says: specialising an interpreter with respect
  to a program yields a compiled version of that program. Eclexia
  already has `eclexia-specialize` (binding-time analysis, partial
  evaluation) -- pushing this further could yield metacompilation.

**Current position:** We chose to keep the explicit pipeline for now
because:

1. Each stage is independently testable and debuggable
2. Students can understand the compilation process step by step
3. The metacompiler approach requires a stable language first

**Recommendation for future work:**

- Short-term: Nanopass decomposition of existing passes
- Medium-term: Truffle/GraalVM as a JIT backend
- Long-term: Self-application / Futamura-style metacompilation

== 8. Lessons from Other Languages

=== 8.1. Algebraic Effects (Koka, OCaml 5)

Koka's evidence-passing translation converts effect handlers into
efficient code without stack manipulation. This maps well to Eclexia's
resource tracking -- effects like `use_energy(amount)` could be
handled as algebraic effects with evidence passing.

=== 8.2. Capability-Based Concurrency (Pony)

Pony's reference capabilities (`iso`, `val`, `ref`, `box`, `tag`, `trn`)
prevent data races at compile time. Given Eclexia's new channel/spawn
system, capability annotations could make concurrency provably safe.

=== 8.3. Quantitative Type Theory (Idris2, Granule)

Types annotated with usage counts (0, 1, many). This maps directly
to resource budgets -- a function that "uses 3 energy" could encode
that in its type signature: `fn(x: Energy^3) -> Result`.

=== 8.4. Structured Concurrency (Trio, Swift, JEP 453)

Instead of bare `spawn`, use structured scopes where child tasks
cannot outlive their parent. This would make resource tracking more
precise -- a scope's total resource usage is the sum of its children.

=== 8.5. Demand-Driven Compilation (rust-analyzer, Salsa)

rust-analyzer showed that demand-driven, incremental compilation
provides the best IDE experience. Eclexia adopted Salsa (the same
framework) for this reason. The key insight: never compute more than
the user needs right now.

== Appendix A: Decision Log

[cols="1,2,3,1"]
|===
| Stage | Decision | Rationale | Section

| Lexer
| Hand-written recursive descent + Pratt
| Error recovery, incremental parsing control
| 1.1

| Parser
| Three-level error recovery
| Prevents cascading errors, good DX
| 1.2

| AST
| Arena allocation
| Cache locality, small references
| 3.2

| Type checker
| Robinson unification + dimensional analysis
| Catches resource errors at compile time
| 2.1, 2.2

| Pipeline
| AST -> HIR -> MIR -> Bytecode -> VM
| Proven architecture, each stage testable
| 3.1

| Interpreter
| Separate tree-walking path
| Fast REPL, debugging reference
| 3.3

| Async runtime
| Tokio wrapper with resource tracking
| Don't rewrite commodity infrastructure
| 4.1

| Parallel iterators
| Simple fork-join over work-stealing
| Transparent, instrumentable, teachable
| 4.2

| Incremental compilation
| Salsa demand-driven framework
| Automatic dependency tracking
| 5.1

| File watching
| notify with debounced batching
| Noise reduction, efficient rebuilds
| 5.2
|===
